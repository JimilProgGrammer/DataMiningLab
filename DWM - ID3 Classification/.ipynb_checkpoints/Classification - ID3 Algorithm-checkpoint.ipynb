{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports section\n",
    "import ast\n",
    "import csv\n",
    "import sys\n",
    "import math\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function that loads csv file and creates a dictionary that contains name of attributes\n",
    "# and maps attributes to indexes and indexes to attributes for further processing.\n",
    "def load_csv_to_header_data(filename):\n",
    "    fpath = os.path.join(os.getcwd(), filename)\n",
    "    fs = csv.reader(open(fpath, newline='\\n'))\n",
    "\n",
    "    all_row = []\n",
    "    for r in fs:\n",
    "        all_row.append(r)\n",
    "\n",
    "    headers = all_row[0]\n",
    "    idx_to_name, name_to_idx = get_header_name_to_idx_maps(headers)\n",
    "\n",
    "    data = {\n",
    "        'header': headers,\n",
    "        'rows': all_row[1:],\n",
    "        'name_to_idx': name_to_idx,\n",
    "        'idx_to_name': idx_to_name\n",
    "    }\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility function that maps each header attrubute to an index and also maps the index to the header\n",
    "# attribute.\n",
    "def get_header_name_to_idx_maps(headers):\n",
    "    name_to_idx = {}\n",
    "    idx_to_name = {}\n",
    "    for i in range(0, len(headers)):\n",
    "        name_to_idx[headers[i]] = i\n",
    "        idx_to_name[i] = headers[i]\n",
    "    return idx_to_name, name_to_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility function that removes those columns from the dataset that are not going to be used during classification\n",
    "# i.e. those columns that are not mentioned in the .cfg file.\n",
    "def project_columns(data, columns_to_project):\n",
    "    data_h = list(data['header'])\n",
    "    data_r = list(data['rows'])\n",
    "\n",
    "    all_cols = list(range(0, len(data_h)))\n",
    "\n",
    "    columns_to_project_ix = [data['name_to_idx'][name] for name in columns_to_project]\n",
    "    columns_to_remove = [cidx for cidx in all_cols if cidx not in columns_to_project_ix]\n",
    "\n",
    "    for delc in sorted(columns_to_remove, reverse=True):\n",
    "        del data_h[delc]\n",
    "        for r in data_r:\n",
    "            del r[delc]\n",
    "\n",
    "    idx_to_name, name_to_idx = get_header_name_to_idx_maps(data_h)\n",
    "\n",
    "    return {'header': data_h, 'rows': data_r,\n",
    "            'name_to_idx': name_to_idx,\n",
    "            'idx_to_name': idx_to_name}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get unique values for each attribute.\n",
    "def get_uniq_values(data):\n",
    "    idx_to_name = data['idx_to_name']\n",
    "    idxs = idx_to_name.keys()\n",
    "\n",
    "    val_map = {}\n",
    "    for idx in iter(idxs):\n",
    "        val_map[idx_to_name[idx]] = set()\n",
    "\n",
    "    for data_row in data['rows']:\n",
    "        for idx in idx_to_name.keys():\n",
    "            att_name = idx_to_name[idx]\n",
    "            val = data_row[idx]\n",
    "            if val not in val_map.keys():\n",
    "                val_map[att_name].add(val)\n",
    "    return val_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Returns count of class label for each attribute.\n",
    "def get_class_labels(data, target_attribute):\n",
    "    rows = data['rows']\n",
    "    col_idx = data['name_to_idx'][target_attribute]\n",
    "    labels = {}\n",
    "    for r in rows:\n",
    "        val = r[col_idx]\n",
    "        if val in labels:\n",
    "            labels[val] = labels[val] + 1\n",
    "        else:\n",
    "            labels[val] = 1\n",
    "    return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get entropy for attribute\n",
    "def entropy(n, labels):\n",
    "    ent = 0\n",
    "    for label in labels.keys():\n",
    "        p_x = labels[label] / n\n",
    "        ent += - p_x * math.log(p_x, 2)\n",
    "    return ent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Partitions the original dataset at the splitting attribute\n",
    "def partition_data(data, group_att):\n",
    "    partitions = {}\n",
    "    data_rows = data['rows']\n",
    "    partition_att_idx = data['name_to_idx'][group_att]\n",
    "    for row in data_rows:\n",
    "        row_val = row[partition_att_idx]\n",
    "        if row_val not in partitions.keys():\n",
    "            partitions[row_val] = {\n",
    "                'name_to_idx': data['name_to_idx'],\n",
    "                'idx_to_name': data['idx_to_name'],\n",
    "                'rows': list()\n",
    "            }\n",
    "        partitions[row_val]['rows'].append(row)\n",
    "    return partitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Partitions the dataset and returns average entropy\n",
    "def avg_entropy_w_partitions(data, splitting_att, target_attribute):\n",
    "    # find uniq values of splitting att\n",
    "    data_rows = data['rows']\n",
    "    n = len(data_rows)\n",
    "    partitions = partition_data(data, splitting_att)\n",
    "\n",
    "    avg_ent = 0\n",
    "\n",
    "    for partition_key in partitions.keys():\n",
    "        partitioned_data = partitions[partition_key]\n",
    "        partition_n = len(partitioned_data['rows'])\n",
    "        partition_labels = get_class_labels(partitioned_data, target_attribute)\n",
    "        partition_entropy = entropy(partition_n, partition_labels)\n",
    "        avg_ent += partition_n / n * partition_entropy\n",
    "\n",
    "    return avg_ent, partitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Returns the most common label.\n",
    "def most_common_label(labels):\n",
    "    mcl = max(labels, key=lambda k: labels[k])\n",
    "    return mcl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function that implements the ID3 classification recursively.\n",
    "def id3(data, uniqs, remaining_atts, target_attribute):\n",
    "    labels = get_class_labels(data, target_attribute)\n",
    "\n",
    "    node = {}\n",
    "\n",
    "    if len(labels.keys()) == 1:\n",
    "        node['label'] = next(iter(labels.keys()))\n",
    "        return node\n",
    "\n",
    "    if len(remaining_atts) == 0:\n",
    "        node['label'] = most_common_label(labels)\n",
    "        return node\n",
    "\n",
    "    n = len(data['rows'])\n",
    "    ent = entropy(n, labels)\n",
    "\n",
    "    max_info_gain = None\n",
    "    max_info_gain_att = None\n",
    "    max_info_gain_partitions = None\n",
    "\n",
    "    for remaining_att in remaining_atts:\n",
    "        avg_ent, partitions = avg_entropy_w_partitions(data, remaining_att, target_attribute)\n",
    "        info_gain = ent - avg_ent\n",
    "        if max_info_gain is None or info_gain > max_info_gain:\n",
    "            max_info_gain = info_gain\n",
    "            max_info_gain_att = remaining_att\n",
    "            max_info_gain_partitions = partitions\n",
    "\n",
    "    if max_info_gain is None:\n",
    "        node['label'] = most_common_label(labels)\n",
    "        return node\n",
    "\n",
    "    node['attribute'] = max_info_gain_att\n",
    "    node['nodes'] = {}\n",
    "\n",
    "    remaining_atts_for_subtrees = set(remaining_atts)\n",
    "    remaining_atts_for_subtrees.discard(max_info_gain_att)\n",
    "\n",
    "    uniq_att_values = uniqs[max_info_gain_att]\n",
    "\n",
    "    for att_value in uniq_att_values:\n",
    "        if att_value not in max_info_gain_partitions.keys():\n",
    "            node['nodes'][att_value] = {'label': most_common_label(labels)}\n",
    "            continue\n",
    "        partition = max_info_gain_partitions[att_value]\n",
    "        node['nodes'][att_value] = id3(partition, uniqs, remaining_atts_for_subtrees, target_attribute)\n",
    "\n",
    "    return node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to load .cfg file that contains metadata about the dataset\n",
    "def load_config(config_file):\n",
    "    with open(config_file, 'r') as myfile:\n",
    "        data = myfile.read().replace('\\n', '')\n",
    "    return ast.literal_eval(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prints the IF...THEN rules generated by the classifier.\n",
    "def pretty_print_tree(root):\n",
    "    stack = []\n",
    "    rules = set()\n",
    "\n",
    "    def traverse(node, stack, rules):\n",
    "        if 'label' in node:\n",
    "            stack.append(' THEN ' + node['label'])\n",
    "            rules.add(''.join(stack))\n",
    "            stack.pop()\n",
    "        elif 'attribute' in node:\n",
    "            ifnd = 'IF ' if not stack else ' AND '\n",
    "            stack.append(ifnd + node['attribute'] + ' EQUALS ')\n",
    "            for subnode_key in node['nodes']:\n",
    "                stack.append(subnode_key)\n",
    "                traverse(node['nodes'][subnode_key], stack, rules)\n",
    "                stack.pop()\n",
    "            stack.pop()\n",
    "\n",
    "    traverse(root, stack, rules)\n",
    "    print(os.linesep.join(rules))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Driver function\n",
    "def main():\n",
    "    config = load_config(\"./config.cfg\")\n",
    "\n",
    "    data = load_csv_to_header_data(config['data_file'])\n",
    "    data = project_columns(data, config['data_project_columns'])\n",
    "    \n",
    "    target_attribute = config['target_attribute']\n",
    "    remaining_attributes = set(data['header'])\n",
    "    remaining_attributes.remove(target_attribute)\n",
    "\n",
    "    uniqs = get_uniq_values(data)\n",
    "\n",
    "    root = id3(data, uniqs, remaining_attributes, target_attribute)\n",
    "\n",
    "    pretty_print_tree(root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IF outlook EQUALS overcast THEN yes\n",
      "IF outlook EQUALS rainy AND windy EQUALS false THEN yes\n",
      "IF outlook EQUALS rainy AND windy EQUALS true THEN no\n",
      "IF outlook EQUALS sunny AND humidity EQUALS high THEN no\n",
      "IF outlook EQUALS sunny AND humidity EQUALS normal THEN yes\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\": main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
